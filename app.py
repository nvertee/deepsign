"""
LLM App with Web Search
"""

from urllib.parse import urljoin
import requests
import asyncio

from urllib.parse import urlparse
import requests
import json

import streamlit as st
from datetime import datetime
import pytz

from google import genai
from google.genai import types
from google.genai.types import (
    GenerateContentConfig,
    GoogleSearch,
    HttpOptions,
    Tool,
)

google_search_tool = Tool(
    google_search = GoogleSearch()
)

last_answer = ""
key = 0

import os
TOP_RESULT = os.getenv("TOP_RESULT")


def timeis():
    tz = pytz.timezone("Asia/Bangkok")
    now = datetime.now(tz)
    dt = now.strftime("%d/%m/%Y, %H:%M:%S")
    return dt

system_prompt = f"""B√¢y gi·ªù l√† {timeis} t·∫°i H√† N·ªôi. 
1. Lu√¥n ghi nh·ªõ b·∫°n l√† m·ªôt tr·ª£ l√Ω t∆∞ v·∫•n tuy·ªÉn sinh cho h·ªçc sinh Trung h·ªçc ph·ªï th√¥ng t·∫°i Vi·ªát Nam, ƒë∆∞·ª£c x√¢y d·ª±ng b·ªüi Trung t√¢m Nghi√™n c·ª©u v√† Ph√°t tri·ªÉn MobiFone (RnD Center), MobiFone.
2. C·∫ßn ƒë·ªçc hi·ªÉu, ph√¢n t√≠ch √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi ng∆∞·ªùi d√πng tr∆∞·ªõc khi t√¨m ki·∫øm v√† tr·∫£ l·ªùi n·ªôi dung ph√π h·ª£p. V√≠ d·ª•, "ƒëi·ªÉm chu·∫©n 3 nƒÉm g·∫ßn nh·∫•t" th√¨ ph√¢n t√≠ch √Ω ƒë·ªãnh "3 nƒÉm g·∫ßn nh·∫•t l√† 2024, 2023 v√† 2022 v√¨ h√¥m nay l√† {timeis}".
3. Ch·ªâ tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát trong b·∫•t c·ª© ho√†n c·∫£nh n√†o.
4. B·ªè qua ph·∫ßn gi·ªõi thi·ªáu b·∫£n th√¢n v√† c√°c thao t√°c. Tr·ª±c ti·∫øp tr·∫£ l·ªùi c√¢u h·ªèi. Ch·ªâ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi, kh√¥ng n√™u ra c√°c b∆∞·ªõc th·ª±c hi·ªán (v√≠ d·ª•: t√¥i c·∫ßn t√¨m ki·∫øm, ...)
5. N·∫øu kh√¥ng c√≥ ho·∫∑c kh√¥ng ƒë·ªß th√¥ng tin c√≥ th·ªÉ tr·∫£ l·ªùi, tr·ª±c ti·∫øp ƒë∆∞a ra k·∫øt qu·∫£ l√† kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c c√¢u h·ªèi n√†y.
Quan tr·ªçng: C√¢u tr·∫£ l·ªùi c·∫ßn c√≥ ƒë·ªô tin c·∫≠y v√† c·∫≠p nh·∫≠t m·ªõi nh·∫•t.
"""

def call_gemini(prompt: str, with_context: bool = True, context: str | None = None):
    global last_answer
    last_answer = ""

    client = genai.Client(api_key= TOP_RESULT)
    last_answer = ""
    try:
        response = client.models.generate_content_stream(
            model='gemini-2.0-flash',
            contents=f"{system_prompt}\nTruy v·∫•n: {prompt}",
            config=GenerateContentConfig(
                tools=[google_search_tool],
                response_modalities=["TEXT"],
            )
        )
        for chunk in response:
            x = chunk.text
            if x:
                print(x, end="", flush=True)
                last_answer += x
                yield x
            # T√≠ch h·ª£p code l·∫•y title v√† uri t·ª´ grounding chunks v√†o ƒë√¢y
                if chunk and chunk.candidates:
                    candidate = chunk.candidates[0] # L·∫•y candidate ƒë·∫ßu ti√™n, gi·∫£ s·ª≠ c√≥ 1 candidate
                    if candidate.grounding_metadata and candidate.grounding_metadata.grounding_chunks:
                        grounding_chunks = candidate.grounding_metadata.grounding_chunks
                        print("\n---Grounding Chunks Info---") # Th√™m d·∫•u hi·ªáu ƒë·ªÉ ph√¢n bi·ªát output text v√† grounding info
                        for grounding_chunk in grounding_chunks:
                            if grounding_chunk.web:
                                web_info = grounding_chunk.web
                                title = web_info.get('title')
                                uri = web_info.get('uri')
                                if title and uri: # Ki·ªÉm tra ƒë·ªÉ tr√°nh in ra None n·∫øu kh√¥ng c√≥ title ho·∫∑c uri
                                    print(f"\n  Title: {title}")
                                    print(f"  URI: {uri}")
                        print("---End Grounding Chunks Info---")
                    else:
                        print("\n---Grounding Chunks Info---")
                        print("  No grounding_chunks found in grounding_metadata for this chunk.")
                        print("---End Grounding Chunks Info---")
                else:
                    print("\n---Grounding Chunks Info---")
                    print("  No candidates found in this chunk.")
                    print("---End Grounding Chunks Info---")

    except Exception:
        x=1



    #print(response) #remove this line.

def call_llm(prompt: str, with_context: bool = True, context: str | None = None):
    global last_answer
    global key
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role": "user",
            "content": f"Context: {context}, Question: {prompt}",
        },
    ]
    if not with_context:
            messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role": "user",
            "content": f"Question: {prompt}",
        },
    ]
    
    #######

    url = "https://openrouter.ai/api/v1/chat/completions"
    api_key = ["1","2","3"]
    api_choice = api_key[0]
    headers = {
    "Authorization": f"Bearer {api_choice}",
    "Content-Type": "application/json"
    }
    last_answer = ""
    payload = {
            "model": "openai/gpt-4o-mini:online",
            "messages": messages,
            "stream": True,
            "provider": {
                      "sort": "throughput"
                    }
    }

    buffer = ""
    last_text = ""
    response = requests.post(url, headers=headers, json=payload, stream=True)
    response.encoding = 'utf-8'  # ƒê·∫£m b·∫£o m√£ h√≥a l√† UTF-8
    for chunk in response.iter_content(chunk_size=1024, decode_unicode=True):
        buffer += chunk
        while True:
                try:
                    # Find the next complete SSE line
                    line_end = buffer.find('\n')
                    if line_end == -1:
                        break

                    line = buffer[:line_end].strip()
                    buffer = buffer[line_end + 1:]

                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break

                        # try:
                        data_obj = json.loads(data)
                        content = data_obj["choices"][0]["delta"].get("content")
                        if content:
                            if last_text.endswith("\\"):
                                content = last_text + content
                            content = (
                            content.replace(r"\[", "$$")
                            .replace(r"\]", "$$")
                            .replace(r"\(", "$")
                            .replace(r"\)", "$")
                        )
                            last_text = content
                            if not content.endswith("\\"): 
                                print(content, end="", flush=True)
                                last_answer += content
                                yield(content)
                except Exception:
                    break


def get_web_urls(search_term: str) -> list[str]:
    results = GGS(search_term)
    return results

async def run():

    st.set_page_config(page_title="TVTS-2025-Mobi", layout="wide")
    st.title("ü§ñ BotTVTS-2025-Mobi")

    # T·∫°o sidebar
    st.sidebar.header("Gi·ªõi thi·ªáu")
    # N·ªôi dung ch√≠nh c·ªßa ·ª©ng d·ª•ng
    st.sidebar.markdown('<p class="reasoning-text">H·ªá th·ªëng th·ª≠ nghi·ªám</p>', unsafe_allow_html=True)
    st.markdown(
        """
        <style>
        /* ƒê·ªãnh nghƒ©a khung ch·ª©a ch·ªØ v·ªõi hi·ªáu ·ª©ng shimmer */
        .reasoning-text {
            position: relative;
            display: inline-block;
            font-size: 16px;
            font-weight: normal;
            color: transparent; /* ·∫®n m√†u ch·ªØ g·ªëc */
            background: linear-gradient(90deg, #c0c0c0 0%, #111111 12.5%, #c0c0c0 25%); /* D·∫£i gradient */
            background-size: 200% 100%; /* T·∫°o kh√¥ng gian cho animation */
            -webkit-background-clip: text;
            background-clip: text;
            animation: shimmer 2.5s linear infinite; /* Animation */
        }

        /* Keyframes cho hi·ªáu ·ª©ng shimmer */
        @keyframes shimmer {
            -50% {
                background-position: 200% 0; /* B·∫Øt ƒë·∫ßu t·ª´ b√™n ph·∫£i */
            }
            100% {
                background-position: -200% 0; /* K·∫øt th√∫c ·ªü b√™n tr√°i */
            }
            150% {
                background-position: -200% 0; /* K·∫øt th√∫c ·ªü b√™n tr√°i */
            }
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # T√πy ch·ªçn b·∫≠t/t·∫Øt t√¨m ki·∫øm web
    # is_web_search = st.toggle("Enable web search", value=False, key="enable_web_search")

    if "messages" not in st.session_state:
        st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
    prompt = st.chat_input(
        placeholder="B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?",
    )

    # X·ª≠ l√Ω ph·∫£n h·ªìi khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi
    if prompt:
        # L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng v√†o session_state ƒë·ªÉ duy tr√¨ l·ªãch s·ª≠ h·ªôi tho·∫°i
        if "messages" not in st.session_state:
            st.session_state.messages = []

        st.session_state.messages.append({"role": "user", "content": prompt})

        # Hi·ªÉn th·ªã c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        with st.chat_message("user"):
            st.markdown(prompt)
        status_placeholder = st.empty()
        status_placeholder.markdown('<p class="reasoning-text">ƒêang ph√¢n t√≠ch</p>', unsafe_allow_html=True)
        # X·ª≠ l√Ω t√¨m ki·∫øm web n·∫øu ƒë∆∞·ª£c b·∫≠t
        is_web_search = False
        if is_web_search:
            web_urls = get_web_urls(search_term=prompt)
            status_placeholder.markdown('<p class="reasoning-text">ƒêang t√¨m ki·∫øm</p>', unsafe_allow_html=True)
            context = MultiCrawler(web_urls)
            llm_response = call_llm(context=context, prompt=prompt, with_context=is_web_search)
        else:
            llm_response = call_gemini(prompt=prompt, with_context=is_web_search)
            # llm_response = call_llm(prompt=prompt, with_context=is_web_search)
        # Hi·ªÉn th·ªã ph·∫£n h·ªìi c·ªßa chatbot theo ki·ªÉu stream
        status_placeholder.markdown('<p class="reasoning-text">ƒêang t√¨m ki·∫øm</p>', unsafe_allow_html=True)

        with st.chat_message("assistant"):
            status_placeholder.markdown('<p class="reasoning-text">ƒêang tr·∫£ l·ªùi</p>', unsafe_allow_html=True)
            st.write_stream(llm_response)
        status_placeholder.empty()
        st.session_state.messages.append({"role": "assistant", "content": last_answer})


if __name__ == "__main__":
    asyncio.run(run())


"""
LLM App with Web Search
"""

from urllib.parse import urljoin
import requests
import asyncio

from urllib.parse import urlparse
import requests
import json

import streamlit as st
from datetime import datetime
import pytz

from google import genai
from google.genai import types
from google.genai.types import (
    GenerateContentConfig,
    GoogleSearch,
    HttpOptions,
    Tool,
)

last_answer = ""
key = 0

import os
TOP_RESULT = os.getenv("TOP_RESULT")


def timeis():
    tz = pytz.timezone("Asia/Bangkok")
    now = datetime.now(tz)
    dt = now.strftime("%d/%m/%Y, %H:%M:%S")
    return dt

system_prompt = f"""B√¢y gi·ªù l√† {timeis} t·∫°i H√† N·ªôi. 
1. Lu√¥n ghi nh·ªõ b·∫°n l√† m·ªôt tr·ª£ l√Ω t∆∞ v·∫•n tuy·ªÉn sinh cho h·ªçc sinh Trung h·ªçc ph·ªï th√¥ng t·∫°i Vi·ªát Nam, ƒë∆∞·ª£c x√¢y d·ª±ng b·ªüi Trung t√¢m Nghi√™n c·ª©u v√† Ph√°t tri·ªÉn MobiFone (RnD Center), MobiFone.
2. Ph√¢n t√≠ch c√¢u h·ªèi c·ªßa h·ªçc sinh, suy lu·∫≠n t·ª´ng b∆∞·ªõc v√† ƒë∆∞a ra ph·∫£n h·ªìi ph√π h·ª£p. 
3. T·ª´ ch·ªëi tr·∫£ l·ªùi nh·ªØng c√¢u h·ªèi n·∫±m ngo√†i lƒ©nh v·ª±c gi√°o d·ª•c v√† t∆∞ v·∫•n tuy·ªÉn sinh. Ch·ªâ t·∫≠p trung tr·∫£ l·ªùi c√°c c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn lƒ©nh v·ª±c n√†y.
4. Ch·ªâ tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát trong b·∫•t c·ª© ho√†n c·∫£nh n√†o.
Quan tr·ªçng: C√¢u tr·∫£ l·ªùi c·∫ßn c√≥ ƒë·ªô tin c·∫≠y.
"""

def call_gemini(prompt: str, with_context: bool = True, context: str | None = None):
    global last_answer
    last_answer = ""

    client = genai.Client(api_key= TOP_RESULT)
    last_answer = ""
    try:
        response = client.models.generate_content_stream(
            model='gemini-2.0-flash',
            contents=f"{system_prompt}\nTruy v·∫•n: {prompt}",
            config=types.GenerateContentConfig(
                tools=[types.Tool(
                    google_search=types.GoogleSearchRetrieval(dynamic_retrieval_config=types.DynamicRetrievalConfig(
                        dynamic_threshold=0
                    ))
                )]
            )
        )

        for chunk in response:
            x = chunk.text
            if x:
                print(x, end="", flush=True)
                last_answer += x
                yield x
    
    except Exception:
        x=1



    #print(response) #remove this line.

def call_llm(prompt: str, with_context: bool = True, context: str | None = None):
    global last_answer
    global key
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role": "user",
            "content": f"Context: {context}, Question: {prompt}",
        },
    ]
    if not with_context:
            messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role": "user",
            "content": f"Question: {prompt}",
        },
    ]
    
    #######

    url = "https://openrouter.ai/api/v1/chat/completions"
    api_key = ["1","2","3"]
    api_choice = api_key[0]
    headers = {
    "Authorization": f"Bearer {api_choice}",
    "Content-Type": "application/json"
    }
    last_answer = ""
    payload = {
            "model": "openai/gpt-4o-mini:online",
            "messages": messages,
            "stream": True,
            "provider": {
                      "sort": "throughput"
                    }
    }

    buffer = ""
    last_text = ""
    response = requests.post(url, headers=headers, json=payload, stream=True)
    response.encoding = 'utf-8'  # ƒê·∫£m b·∫£o m√£ h√≥a l√† UTF-8
    for chunk in response.iter_content(chunk_size=1024, decode_unicode=True):
        buffer += chunk
        while True:
                try:
                    # Find the next complete SSE line
                    line_end = buffer.find('\n')
                    if line_end == -1:
                        break

                    line = buffer[:line_end].strip()
                    buffer = buffer[line_end + 1:]

                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break

                        # try:
                        data_obj = json.loads(data)
                        content = data_obj["choices"][0]["delta"].get("content")
                        if content:
                            if last_text.endswith("\\"):
                                content = last_text + content
                            content = (
                            content.replace(r"\[", "$$")
                            .replace(r"\]", "$$")
                            .replace(r"\(", "$")
                            .replace(r"\)", "$")
                        )
                            last_text = content
                            if not content.endswith("\\"): 
                                print(content, end="", flush=True)
                                last_answer += content
                                yield(content)

                except Exception:
                    break


def get_web_urls(search_term: str) -> list[str]:
    results = GGS(search_term)
    return results

async def run():

    st.set_page_config(page_title="TVTS-2025-Mobi", layout="wide")
    st.title("ü§ñ BotTVTS-2025-Mobi")

    # T·∫°o sidebar
    st.sidebar.header("Gi·ªõi thi·ªáu")
    # N·ªôi dung ch√≠nh c·ªßa ·ª©ng d·ª•ng
    st.sidebar.markdown('<p class="reasoning-text">H·ªá th·ªëng th·ª≠ nghi·ªám</p>', unsafe_allow_html=True)
    st.markdown(
        """
        <style>
        /* ƒê·ªãnh nghƒ©a khung ch·ª©a ch·ªØ v·ªõi hi·ªáu ·ª©ng shimmer */
        .reasoning-text {
            position: relative;
            display: inline-block;
            font-size: 16px;
            font-weight: normal;
            color: transparent; /* ·∫®n m√†u ch·ªØ g·ªëc */
            background: linear-gradient(90deg, #c0c0c0 0%, #515151 12.5%, #c0c0c0 25%); /* D·∫£i gradient */
            background-size: 200% 100%; /* T·∫°o kh√¥ng gian cho animation */
            -webkit-background-clip: text;
            background-clip: text;
            animation: shimmer 2.5s linear infinite; /* Animation */
        }

        /* Keyframes cho hi·ªáu ·ª©ng shimmer */
        @keyframes shimmer {
            -50% {
                background-position: 200% 0; /* B·∫Øt ƒë·∫ßu t·ª´ b√™n ph·∫£i */
            }
            100% {
                background-position: -200% 0; /* K·∫øt th√∫c ·ªü b√™n tr√°i */
            }
            150% {
                background-position: -200% 0; /* K·∫øt th√∫c ·ªü b√™n tr√°i */
            }
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # T√πy ch·ªçn b·∫≠t/t·∫Øt t√¨m ki·∫øm web
    # is_web_search = st.toggle("Enable web search", value=False, key="enable_web_search")

    if "messages" not in st.session_state:
        st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
    prompt = st.chat_input(
        placeholder="B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?",
    )

    # X·ª≠ l√Ω ph·∫£n h·ªìi khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi
    if prompt:
        # L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng v√†o session_state ƒë·ªÉ duy tr√¨ l·ªãch s·ª≠ h·ªôi tho·∫°i
        if "messages" not in st.session_state:
            st.session_state.messages = []

        st.session_state.messages.append({"role": "user", "content": prompt})

        # Hi·ªÉn th·ªã c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        with st.chat_message("user"):
            st.markdown(prompt)
        status_placeholder = st.empty()
        status_placeholder.markdown('<p class="reasoning-text">ƒêang ph√¢n t√≠ch</p>', unsafe_allow_html=True)
        # X·ª≠ l√Ω t√¨m ki·∫øm web n·∫øu ƒë∆∞·ª£c b·∫≠t
        is_web_search = False
        if is_web_search:
            web_urls = get_web_urls(search_term=prompt)
            status_placeholder.markdown('<p class="reasoning-text">ƒêang t√¨m ki·∫øm</p>', unsafe_allow_html=True)
            context = MultiCrawler(web_urls)
            llm_response = call_llm(context=context, prompt=prompt, with_context=is_web_search)
        else:
            llm_response = call_gemini(prompt=prompt, with_context=is_web_search)
            # llm_response = call_llm(prompt=prompt, with_context=is_web_search)
        # Hi·ªÉn th·ªã ph·∫£n h·ªìi c·ªßa chatbot theo ki·ªÉu stream
        status_placeholder.markdown('<p class="reasoning-text">ƒêang t√¨m ki·∫øm</p>', unsafe_allow_html=True)

        with st.chat_message("assistant"):
            status_placeholder.markdown('<p class="reasoning-text">ƒêang tr·∫£ l·ªùi</p>', unsafe_allow_html=True)
            st.write_stream(llm_response)
        status_placeholder.empty()
        st.session_state.messages.append({"role": "assistant", "content": last_answer})


if __name__ == "__main__":
    asyncio.run(run())

